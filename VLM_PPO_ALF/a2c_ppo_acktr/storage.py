import torch
from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler
from tqdm import tqdm


def _flatten_helper(T, N, _tensor):
    return _tensor.view(T * N, *_tensor.size()[2:])


class RolloutStorage(object):
    def __init__(self, num_steps, num_processes, obs_shape, action_space, max_new_tokens):
        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)
        #hard-code to cases of max_new_tokens being smaller than 32
        self.output_ids = torch.zeros(
            num_steps, num_processes, 2*max_new_tokens).long()
        self.rewards = torch.zeros(num_steps, num_processes, 1)
        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)
        self.returns = torch.zeros(num_steps + 1, num_processes, 1)
        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)
        if action_space.__class__.__name__ == 'Discrete':
            action_shape = 1
        else:
            action_shape = action_space.shape[0]
        self.actions = torch.zeros(num_steps, num_processes, action_shape)
        if action_space.__class__.__name__ == 'Discrete':
            self.actions = self.actions.long()
        self.masks = torch.ones(num_steps + 1, num_processes, 1)

        # Masks that indicate whether it's a true terminal state
        # or time limit end state
        self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)

        self.num_steps = num_steps
        self.step = 0

    def to(self, device):
        self.obs = self.obs.to(device)
        self.output_ids = self.output_ids.to(device)
        self.rewards = self.rewards.to(device)
        self.value_preds = self.value_preds.to(device)
        self.returns = self.returns.to(device)
        self.action_log_probs = self.action_log_probs.to(device)
        self.actions = self.actions.to(device)
        self.masks = self.masks.to(device)
        self.bad_masks = self.bad_masks.to(device)

    def insert(self, obs, output_ids, actions, action_log_probs,
               value_preds, rewards, masks, bad_masks):
        self.obs[self.step + 1].copy_(obs)
        self.output_ids[self.step].copy_(output_ids)
        self.actions[self.step].copy_(actions)
        self.action_log_probs[self.step].copy_(action_log_probs)
        self.value_preds[self.step].copy_(value_preds)
        self.rewards[self.step].copy_(rewards)
        self.masks[self.step + 1].copy_(masks)
        self.bad_masks[self.step + 1].copy_(bad_masks)

        self.step = (self.step + 1) % self.num_steps

    def after_update(self):
        self.obs[0].copy_(self.obs[-1])
        self.masks[0].copy_(self.masks[-1])
        self.bad_masks[0].copy_(self.bad_masks[-1])

    def compute_returns(self,
                        next_value,
                        use_gae,
                        gamma,
                        gae_lambda,
                        use_proper_time_limits=True):
        if use_proper_time_limits:
            if use_gae:
                self.value_preds[-1] = next_value
                gae = 0
                for step in reversed(range(self.rewards.size(0))):
                    delta = self.rewards[step] + gamma * self.value_preds[
                        step + 1] * self.masks[step +
                                               1] - self.value_preds[step]
                    gae = delta + gamma * gae_lambda * self.masks[step +
                                                                  1] * gae
                    gae = gae * self.bad_masks[step + 1]
                    self.returns[step] = gae + self.value_preds[step]
            else:
                self.returns[-1] = next_value
                for step in reversed(range(self.rewards.size(0))):
                    self.returns[step] = (self.returns[step + 1] * \
                        gamma * self.masks[step + 1] + self.rewards[step]) * self.bad_masks[step + 1] \
                        + (1 - self.bad_masks[step + 1]) * self.value_preds[step]
        else:
            if use_gae:
                self.value_preds[-1] = next_value
                gae = 0
                for step in reversed(range(self.rewards.size(0))):
                    delta = self.rewards[step] + gamma * self.value_preds[
                        step + 1] * self.masks[step +
                                               1] - self.value_preds[step]
                    gae = delta + gamma * gae_lambda * self.masks[step +
                                                                  1] * gae
                    self.returns[step] = gae + self.value_preds[step]
            else:
                self.returns[-1] = next_value
                for step in reversed(range(self.rewards.size(0))):
                    self.returns[step] = self.returns[step + 1] * \
                        gamma * self.masks[step + 1] + self.rewards[step]

    def feed_forward_generator(self,
                               advantages,
                               mini_batch_size=None):
        num_steps, num_processes = self.rewards.size()[0:2]
        batch_size = num_processes * num_steps

        sampler = BatchSampler(
            SubsetRandomSampler(range(batch_size)),
            mini_batch_size,
            drop_last=True)
        for indices in sampler:
            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]
            actions_batch = self.actions.view(-1,
                                              self.actions.size(-1))[indices]
            output_ids_batch = self.output_ids.view(-1,
                                              self.output_ids.size(-1))[indices]
            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]
            return_batch = self.returns[:-1].view(-1, 1)[indices]
            masks_batch = self.masks[:-1].view(-1, 1)[indices]
            old_action_log_probs_batch = self.action_log_probs.view(-1,
                                                                    1)[indices]
            if advantages is None:
                adv_targ = None
            else:
                adv_targ = advantages.view(-1, 1)[indices]

            yield obs_batch, output_ids_batch, actions_batch, \
                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ




# 240825tra # 20240830dic
def find_first_diff(list1, list2):
    # @TODO: ËÄÉËôëÊúÄÂêé‰∏Ä‰∏™ËæìÂÖ•ÁöÑÂä®‰Ωú
    # @TODO: ËÄÉËôëÁõ∏ÂêåÂä®‰Ωú‰∏çÂêåÁä∂ÊÄÅËΩ¨Áßªüåü    
    # Á°Æ‰øùËæìÂÖ•ÊòØÂàóË°®
    if not isinstance(list1, list) or not isinstance(list2, list):
        raise TypeError("ËæìÂÖ•ÂøÖÈ°ªÊòØÂàóË°®")
    
    # ÊâæÂà∞ÊúÄÁü≠ÁöÑÂàóË°®ÈïøÂ∫¶
    min_length = min(len(list1), len(list2))

    # ÈÅçÂéÜËá≥ÊúÄÁü≠ÂàóË°®ÁöÑÈïøÂ∫¶ÔºåÊØîËæÉÊØè‰∏™‰ΩçÁΩÆÁöÑÁ¨¨‰∫å‰∏™ÂÖÉÁ¥† (text_obs)
    for i in range(min_length):
        if list1[i][1] != list2[i][1]:
            return i
    
    # Â¶ÇÊûúÊ≤°ÊúâÂèëÁé∞‰∏çÂêåËøîÂõû -1
    return -1

def compare_2_trajs(ta, tb):
    """
    ta & tb: List(
        [obs, text_obs, text_action, success_rate]
    )
    """
    # ÂÆö‰Ωç‰∏§‰∏™ÂàóË°®Âá∫Áé∞Á¨¨‰∏Ä‰∏™text_obsÂÖÉÁ¥†‰∏çÂêåÁöÑÂùêÊ†á
    step_idx = find_first_diff(ta, tb)
    
    if step_idx == -1 or step_idx == 0:  # Â¶ÇÊûúËΩ®ËøπÁõ∏ÂêåÊàñËÄÖÁ¨¨‰∏Ä‰∏™Â∞±‰∏çÂêåÔºåÈÉΩÊ≤°ÊúâÊÑè‰πâ @TODO: ËÄÉËôëÊúÄÂêé‰∏Ä‰∏™pointÂä®‰Ωú‰∏çÂêåüåü
        return ["same", -1]
    
    reward_a, reward_b = float(ta[-1][3]), float(tb[-1][3])
    if reward_a > reward_b:
        return ["better", step_idx]
    elif reward_a < reward_b:
        return ["worse", step_idx]
    else:
        return ["same", step_idx]


def get_preference_data(preference, diff_idx, traA, traB, history_horizon=3):
    """
    Ëøô‰∏™ÂáΩÊï∞Áî®‰∫éÂ∞ÜËΩ®ËøπÂØπËΩ¨Êç¢Êàêprompt + chosen/rejected actionÁöÑÊñπÂºèËøîÂõû
    Input:
        preference: str "better","worse"
        diff_idx: int Á¨¨‰∏Ä‰∏™‰∏çÂêåÂÖÉÁ¥†ÁöÑÁ¥¢Âºï
        traA & traB: List([obs, text_obs, text_action, success_rate, prompt], ...)
    
    Output:
        pre_prompt
        pre_better
        pre_worse
        obs
    """
    # @TODO: ËøôÈáåÂè™ËÄÉËôë‰∫Ütext_obsÁõ∏ÂêåÔºåÂéÜÂè≤Âä®‰ΩúÂ∞±‰∏ÄÂÆöÁõ∏ÂêåÁöÑÊÉÖÂÜµüåü
    # ÁúüÊ≠£Âä®‰Ωú‰∏çÂêåÁöÑÂ∫îËØ•ÊòØÁ¨¨diff_idx - 1‰∏™Âä®‰Ωú
    if diff_idx > 1:
        text_obs_action_pairs = [arr[1] + "\n" + arr[2] for arr in traA[:diff_idx - 2]]
    else:
        text_obs_action_pairs = []
    text_obs_action_pairs = text_obs_action_pairs[-history_horizon:]
    text_obs_action_pairs.append(traA[diff_idx - 1][4]) # ËØ•Ê≠•ÁöÑpromptË¶ÅÊ∑ªÂä†
    pre_prompt_text = '\n'.join(text_obs_action_pairs)

    if preference == "better":
        pre_better_text = traA[diff_idx - 1][2]
        pre_worse_text = traB[diff_idx - 1][2]
    else:
        pre_better_text = traB[diff_idx - 1][2]
        pre_worse_text = traA[diff_idx - 1][2]
    obs = traA[diff_idx - 1][0]
    # print(pre_prompt_text, pre_better_text, pre_worse_text)
    return pre_prompt_text, pre_better_text, pre_worse_text, obs
    
from llava.mm_utils import tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
class TrajBuffer(object):
    def __init__(self, max_pairs, num_processes, max_history_tokens, max_new_tokens, obs_shape, history_horizon=3):
        """
        better_sample = better_obs_batch, better_output_ids_batch
        """
        self.max_pairs = max_pairs
        self.max_history_tokens = max_history_tokens
        self.max_new_tokens = max_new_tokens
        self.buffer = {}
        self.current_init_state = None
        self.current_traj_index = 0
        self.pre_prompt = torch.zeros(max_pairs, num_processes, 2*max_history_tokens).long()
        self.pre_better = torch.zeros(max_pairs, num_processes, 2*max_new_tokens).long()
        self.pre_better_obs = torch.zeros(max_pairs, num_processes, *obs_shape)
        self.pre_worse = torch.zeros(max_pairs, num_processes, 2*max_new_tokens).long()
        self.pre_worse_obs = torch.zeros(max_pairs, num_processes, *obs_shape)  # @TODO: ÁúãÁúãobsÊòØÂê¶‰∏ÄËá¥

        self.history_horizon = history_horizon
        self.valid_pairs = 0  # Ëøô‰∏™ÂèòÈáèÁî®‰∫éÂ≠òÂÇ®Êó¢ÊúâÊï∞ÊçÆÁöÑÊï∞Èáè
        self.saving_index = 0  # Ëøô‰∏™ÂèòÈáèÁî®‰∫éÂæ™ÁéØÊõ¥Êñ∞bufferÁöÑÂ≠òÂÇ®ÂèòÈáè
    
    def start_traj(self, init_text_obs=None):
        """
        Ëøô‰∏™ÂáΩÊï∞ÁöÑ‰ΩúÁî®ÊòØÂêØÂä®‰∏Ä‰∏™Êñ∞ÁöÑËΩ®Ëøπ„ÄÇ
        Âú®main_alf.pyËøêË°åÊó∂Â¶ÇÊûúÊé•Êî∂Âà∞done=True, ÊàñÂºÄÂßãÊñ∞ÁöÑupdateÂæ™ÁéØ, ÂàôË∞ÉÁî®Ê≠§ÂáΩÊï∞„ÄÇ
        Ê†πÊçÆËΩ®ËøπÁöÑinit_observation_text, Â¶ÇÊûúÂ≠òÂú®ÂàôÊ∑ªÂä†Êñ∞ËΩ®Ëøπ, ‰∏çÂ≠òÂú®ÂàôÂàõÂª∫Áõ∏Â∫îÁöÑKEY„ÄÇ
        Êõ¥Êñ∞ÂΩìÂâçËΩ®ËøπÁöÑKEYÂíåIndex (IndexÁî®‰∫éÂÆö‰ΩçtrajsÁöÑList(), ÊåáÁöÑÊòØÂú®trajs‰∏≠ÁöÑÁ¨¨Âá†‰∏™traj)„ÄÇ
        Input: init_observation_text (Str)
        Output: -
        """
        self.current_init_state = init_text_obs
        if init_text_obs in self.buffer:
            self.buffer[init_text_obs].append([])
            self.current_traj_index = len(self.buffer[init_text_obs]) - 1
        else:
            self.buffer[init_text_obs] = [[],]
            self.current_traj_index = 0

    def get_history_data(self):
        """
        Ëøô‰∏™ÂáΩÊï∞ÁöÑ‰ΩúÁî®ÊòØÁî®‰∫épromptÁîüÊàêÁöÑÊó∂ÂÄôÂºïÂÖ•ÂéÜÂè≤ËΩ®Ëøπ‰ø°ÊÅØ
        Input: -
        Output: Str Áî±ÂéÜÂè≤text_obsÂíåtext_actionÁªÑÊàêÊñáÊú¨ÊÆµËêΩ
        """
        try:
            if len(self.buffer[self.current_init_state][self.current_traj_index]) > 0:  # jkc0904
                text_obs_action_pairs = ["text_observation: " + arr[1] + "\naction: " + arr[2] for arr in self.buffer[self.current_init_state][self.current_traj_index][-self.history_horizon:]]
            else:
                return ""
            text_history = '\n'.join(text_obs_action_pairs)
        except Exception as e:
            print(f"\033[31m{e}, please start a trajectory first.\033[0m")
            exit(1)
        return text_history

    def add_new_state(self, obs, text_obs, text_action, success_rate, prompt=None):
        """
        Ëøô‰∏™ÂáΩÊï∞ÁöÑ‰ΩúÁî®ÊòØÂä†ÂÖ•‰∏ÄÁªÑÊï∞ÊçÆÂà∞ÂΩìÂâçËΩ®Ëøπ
        Input:
            obs: tensor([1, 300, 300, 3])
            text_obs: str
            text_action: str
            success_rate: float(1)
            prompt: str Ëøô‰∏™ÊòØÊØè‰∏ÄÊ≠•ÊâÄ‰ΩøÁî®ÁöÑprompt
        Output: -
        """
        self.buffer[self.current_init_state][self.current_traj_index].append([obs, text_obs, text_action, success_rate, prompt])
    
    def add_test_state(self, tokenizer):
        self.start_traj("test")
        self.add_new_state(torch.rand(300, 300, 3), "i see the environment", "look", 0.03, "")
        self.add_new_state(torch.rand(300, 300, 3), "i see table 2", "go to table 2", 0.08, "")
        self.start_traj("test")
        self.add_new_state(torch.rand(300, 300, 3), "i see the environment is big", "asd", 0.03, "")
        self.add_new_state(torch.rand(300, 300, 3), "nothing happens", "go to table 2", 0.0, "")
        self.get_pairs_data(tokenizer)
    
    def get_pairs_data(self, tokenizer):
        """
        ÊûÑÈÄ†Ê†∑Êú¨ÂØπÊï∞ÊçÆ
        ‰ªéself.bufferÈÅçÂéÜËØªÂèñÁõ∏Âêåinit_stateÁöÑËΩ®Ëøπ, ÊûÑÈÄ†Ê†∑Êú¨ÂØπ;
        Â≠òÂÇ®Âà∞self.pre_prompt„ÄÅself.pre_better„ÄÅself.pre_better_obsÂíåself.pre_worse„ÄÅself.pre_worse_obs‰∏≠„ÄÇ
        Input: tokenizer
        Output: _
        """
        # ÈÅçÂéÜÂ≠óÂÖ∏buffer‰∏≠ÊâÄÊúâÂàùÂßãÁä∂ÊÄÅÁõ∏ÂêåÁöÑËΩ®ËøπÂØπ
        for init_stat, trajs in tqdm(self.buffer.items()):
            for i in range(len(trajs) - 1, 0, -1):
                for j in range(i - 1, -1, -1):
                    preference, diff_idx = compare_2_trajs(trajs[i], trajs[j])
                    if preference == "same": 
                        print("same")
                        continue
                    print("valid")
                    # print("valid")
                    pre_prompt_text, pre_better_text, pre_worse_text, obs = get_preference_data(preference, diff_idx, trajs[i], trajs[j], history_horizon=self.history_horizon)
                    pre_prompt = tokenizer_image_token(pre_prompt_text, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0)
                    pre_prompt[pre_prompt == 0] = 259 # 869: . (period), 29871: SPIECE, 259: whitespace
                    # print(pre_prompt.size())

                    pre_better = tokenizer(pre_better_text).input_ids
                    pre_worse = tokenizer(pre_worse_text).input_ids

                    # ÂπøÊí≠Âà∞max_new_tokensÁöÑÈïøÂ∫¶
                    if len(pre_better) < 2 * self.max_new_tokens:
                        pre_better += [0] * (2 * self.max_new_tokens - len(pre_better))
                    if len(pre_worse) < 2 * self.max_new_tokens:
                        pre_worse += [0] * (2 * self.max_new_tokens - len(pre_worse))

                    if pre_prompt.size()[-1] < 2 * self.max_history_tokens:
                        pre_prompt = torch.cat((pre_prompt, torch.zeros(1, 2 * self.max_history_tokens - pre_prompt.size()[-1])), dim=1)

                    pre_better = pre_better[:2 * self.max_new_tokens]
                    pre_worse = pre_worse[:2 * self.max_new_tokens]
                    pre_prompt = pre_prompt[:, -2 * self.max_history_tokens:]  # jkc0904: Ëé∑Âèñprompt‰∏≠Èù†ÂêéÁöÑÂéÜÂè≤üåü  # @TODO: need to check

                    self.pre_prompt[self.saving_index % self.max_pairs].copy_(pre_prompt)

                    self.pre_better[self.saving_index % self.max_pairs].copy_(torch.tensor(pre_better))
                    self.pre_worse[self.saving_index % self.max_pairs].copy_(torch.tensor(pre_worse))
                    self.pre_better_obs[self.saving_index % self.max_pairs].copy_(obs)
                    self.pre_worse_obs[self.saving_index % self.max_pairs].copy_(obs)

                    if self.valid_pairs < self.max_pairs:
                        self.valid_pairs += 1  # Êõ¥Êñ∞Â≠òÂÇ®Êï∞ÊçÆÈáè
                    self.saving_index += 1  # Êõ¥Êñ∞Âæ™ÁéØÂ≠òÂÇ®ÂèòÈáè
                    print(self.valid_pairs)

        
    def feed_forward_generator(self, mini_batch_size=None):
        """
        Áî±self.pre_prompt„ÄÅself.pre_betterÂíåself.pre_worseËØªÂèñÂπ∂yieldÊï∞ÊçÆ
        Output: prompt, better_sampleÂíåworse_sampleÁöÑÊ†∑Êú¨ÁîüÊàêÂô®--generator()
        """
        num_samples = self.valid_pairs


        sampler = BatchSampler(
            SubsetRandomSampler(range(num_samples)),
            mini_batch_size,
            drop_last=True)
        for indices in sampler:
            pre_obs_batch = self.pre_better_obs[:-1].view(-1, *self.pre_better_obs.size()[2:])[indices]
            pre_prompt_batch = self.pre_prompt[:-1].view(-1, self.pre_prompt.size()[-1])[indices]
            pre_better_batch = self.pre_better[:-1].view(-1, self.pre_better.size()[-1])[indices]
            pre_worse_batch = self.pre_worse[:-1].view(-1, self.pre_worse.size()[-1])[indices]
            
            
            yield pre_obs_batch, pre_prompt_batch, pre_better_batch, pre_worse_batch




if __name__ == "__main__":
    import random
    import string
    import torch
    def generate_random_string(length):
        letters = string.ascii_letters  # ÂåÖÂê´ÊâÄÊúâÂ§ßÂ∞èÂÜôÂ≠óÊØç
        return ''.join(random.choice(letters) for i in range(length))


    buffer = TrajBuffer(20, 1, 100, 50, (300, 300, 3))
    buffer.start_traj("haha")
    print(buffer.get_history_data(), "<<!!!!!!!!!!!")
    # max_pairs, num_processes, max_history_tokens, max_new_tokens, obs_shape
    # for i in range(5):
    #     init_stat = generate_random_string(6)
    #     for x in range(6):
    #         buffer.start_traj(init_stat)
    #         obs = torch.rand(5, 5, 3)
    #         buffer.add_new_state(obs, init_stat, generate_random_string(3), float(random.random()), generate_random_string(random.randint(2,7)))
    #         for j in range(7):
    #             obs = torch.rand(5, 5, 3)
    #             text_obs = generate_random_string(14)
    #             success_rate = float(random.random())
    #             text_action = generate_random_string(3)
    #             prompt = generate_random_string(random.randint(2,7))
    #             buffer.add_new_state(obs, text_obs, text_action, success_rate, prompt)
    

    # print(buffer.buffer.keys())
    # print(buffer.buffer)
    # print(buffer.get_history_data())

    from transformers import AutoTokenizer

    # ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÂêçÁß∞Êù•Âä†ËΩΩ‰∏Ä‰∏™Âü∫Á°ÄÁöÑtokenizer
    # ËøôÈáåÊàë‰ª¨‰ΩøÁî®BERTÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã
    tokenizer = AutoTokenizer.from_pretrained("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/jiaokechen/Qwen2-7B-Instruct")
    # print(tokenizer("sadasd 000 0.213123123").input_ids)
    # print(tokenizer("100!0 0").input_ids)
    # print(tokenizer("!").input_ids)

    # buffer.get_pairs_data(tokenizer)
    buffer.add_test_state(tokenizer)
    # print(f"\033[32m{buffer.pre_prompt}\033[0m")

    rollout = buffer.feed_forward_generator(1)
    for obs, pt, pre, rej in rollout:
        print(f"\033[34m{obs.size()}, \033[0m{pt.size()}, \033[32m{pre}, \033[33m{rej.size()}\033[0m\n\n")

