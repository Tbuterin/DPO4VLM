from patch import replace_llama_attn_with_xformers_attn
replace_llama_attn_with_xformers_attn()

import time
from collections import deque

import gymnasium as gym
from gymnasium import spaces
import gym_cards
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torchvision.transforms import ToPILImage  # jkc
import matplotlib.pyplot as plt  # jkc

from a2c_ppo_acktr import algo, utils, rl_utils
from a2c_ppo_acktr.rl_utils import get_prompt, text_projection, get_alfworld_prompt
from a2c_ppo_acktr.rl_utils import get_dpo_prompt  # jkc0904
# from a2c_ppo_acktr.arguments import get_args
from a2c_ppo_acktr.model import VLMPolicy, VLMValue
from a2c_ppo_acktr.storage import RolloutStorage, TrajBuffer  # jkc
from a2c_ppo_acktr.llava_interface import llava_evaluate, llava_generate
from a2c_ppo_acktr.llava_interface import init_pretrained_model, find_all_linear_names, load_lora_model

# For alfworld
from alf_utils import load_config_file, get_obs_image, ALF_ACTION_LIST, process_action, compute_reward, AlfEnv


from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
## IMAGE_TOKEN_INDEX: åœ¨å¤„ç†å›¾åƒæˆ–æ··åˆæ•°æ®ï¼ˆä¾‹å¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰æ—¶ï¼Œç”¨äºŽæŒ‡ç¤ºå›¾åƒæ•°æ®åœ¨æ•´ä¸ªæ•°æ®åºåˆ—ä¸­çš„ç´¢å¼•ä½ç½®ã€‚
## DEFAULT_IMAGE_TOKEN: è¡¨ç¤ºä¸€ä¸ªé»˜è®¤çš„å›¾åƒæ ‡è®°ï¼Œç”¨äºŽåœ¨æ•°æ®åºåˆ—ä¸­æ’å…¥å›¾åƒçš„å ä½ç¬¦ã€‚è¿™é€šå¸¸æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼Œç”¨äºŽä¸Žéžå›¾åƒæ•°æ®ï¼ˆå¦‚æ–‡æœ¬ï¼‰åŒºåˆ†å¼€æ¥ã€‚
## DEFAULT_IM_START_TOKEN: è¡¨ç¤ºå›¾åƒæ•°æ®æ®µçš„èµ·å§‹æ ‡è®°ã€‚è¿™é€šå¸¸ç”¨äºŽæŒ‡ç¤ºåºåˆ—ä¸­å›¾åƒæ•°æ®çš„èµ·å§‹ä½ç½®ï¼Œæ–¹ä¾¿æ¨¡åž‹åœ¨å¤„ç†æ—¶æ­£ç¡®åœ°è¯†åˆ«å’Œå¤„ç†å›¾åƒæ•°æ®ã€‚
## DEFAULT_IM_END_TOKEN: è¡¨ç¤ºå›¾åƒæ•°æ®æ®µçš„ç»“æŸæ ‡è®°ã€‚è¿™ä¸Žèµ·å§‹æ ‡è®°ä¸€èµ·ä½¿ç”¨ï¼Œç”¨äºŽå®šä¹‰å›¾åƒæ•°æ®åœ¨åºåˆ—ä¸­çš„èŒƒå›´ã€‚

from llava.conversation import conv_templates, SeparatorStyle
from llava.model.builder import load_pretrained_model
from llava.utils import disable_torch_init
from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria
from llava.model import LlavaLlamaForCausalLM

from functools import partial
from typing import List, Optional
from peft import LoraConfig, get_peft_model
from transformers import AutoTokenizer, AutoImageProcessor
import transformers

from tqdm import tqdm

import accelerate
from accelerate.state import AcceleratorState

import warnings
warnings.filterwarnings("ignore")

import os
import copy

# jkc edit for DPO
from a2c_ppo_acktr.model import DPOPolicy
from transformers import HfArgumentParser
from configs import (
    H4ArgumentParser,
    ModelArguments,
    DataArguments,
    RLArguments,
    DPOConfig,
    StepDPOConfig
)
from dataclasses import dataclass, field


def torch_init(args):
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)

    if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:
        print(f"\033[32mCUDA Deterministic.\033[0m")
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

    torch.set_num_threads(1)  # é™åˆ¶ PyTorch ï¼ˆåœ¨CPUä¸Šï¼‰åªä½¿ç”¨ä¸€ä¸ªçº¿ç¨‹ï¼Œé€šå¸¸ç”¨äºŽé¿å…å¤šçº¿ç¨‹ç«žäº‰å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚


def load_base_model(args):
    model_path = args.model_path
    cache_dir = args.cache_dir

    # æ‰“å°æ¨¡åž‹è·¯å¾„ã€‚å¦‚æžœè·¯å¾„ä¸­åŒ…å« loraï¼ŒåŠ è½½ LoRA æ¨¡åž‹ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æ”¯æŒ 8bit æˆ– 4bit é‡åŒ–ã€‚å¦‚æžœä¸åŒ…å« loraï¼Œåˆ™åŠ è½½æ ‡å‡†çš„ LLaVA æ¨¡åž‹ï¼Œå¯èƒ½ä½¿ç”¨ 8bit æˆ– 4bit é‡åŒ–ã€‚
    print(f"Path of the model is {model_path}")
    if "lora" in model_path:
        base, tokenizer = load_lora_model(model_path, cache_dir=cache_dir)
        if args.q8 or args.q4:
            raise ValueError("Lora model does not support 8bit or 4bit quantization")
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)
        if args.q8:
            print("8bit quantization")
            base = LlavaLlamaForCausalLM.from_pretrained(model_path, load_in_8bit=True, cache_dir=cache_dir)
        elif args.q4:
            q4_config = transformers.BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type='nf4'
                    )
            print("4bit quantization")
            base = LlavaLlamaForCausalLM.from_pretrained(model_path, load_in_4bit=True, quantization_config=q4_config, cache_dir=cache_dir)
        else:
            base = LlavaLlamaForCausalLM.from_pretrained(model_path, cache_dir=cache_dir)
    return base, tokenizer




def main():
    """
    æ³¨é‡Šäº† get_args()
    æ·»åŠ äº† args, model_args, data_args, training_argsåˆ†åˆ«ä¸ºRLArguments, ModelArguments, DataArguments, StepDPOConfig
    æ›¿æ¢äº† actor_criticç»“æž„ä¸ºpolicy_model, å¹¶æ·»åŠ ref_model = policy_model
    æ›¿æ¢äº† agentè®¾ç½®ä¸ºalg0.DPO
    æ³¨é‡Šäº† åŒç«¯é˜Ÿåˆ—çš„åˆå§‹åŒ–
    æ³¨é‡Šäº† wandbä¸­æ‰€æœ‰valueç›¸å…³çš„å‚æ•°
    """

    ############################################################
    # ä½¿ç”¨ H4ArgumentParser æ¥è§£æžæ¨¡åž‹ã€æ•°æ®å’Œè®­ç»ƒå‚æ•° KEY: addhfparser
    ############################################################
    parser = H4ArgumentParser((RLArguments, ModelArguments, DataArguments, StepDPOConfig))   # jkc0829
    args, model_args, data_args, training_args = parser.parse()   # jkc0829

    ###############
    # torch settings
    ###############
    torch_init(args)

    #########################
    # load model and tokenizer
    #########################
    base, tokenizer = load_base_model(args)  # base: åˆ›å»ºçš„Llavaæ¨¡åž‹
    accelerator = accelerate.Accelerator(gradient_accumulation_steps=args.grad_accum_steps)  # å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¢¯åº¦ç´¯ç§¯  # TODO
    device = accelerator.device
    print(f"\033[33mUsing {device}.\033[0m")
    model_device = device
    base = base.to(model_device)  # jkc0904
    
    print(f"\033[32mModel created.\033[0m")

    base.config.max_length = 1024  # @TODO: ä¿®æ”¹æ›´å¤§å€¼ï¼Œå› ä¸ºåŠ å…¥äº†åŽ†å²æ•°æ®
    print(f"\033[33mModel max context length:\033[0m{base.config.max_length}")
    base, tokenizer = init_pretrained_model(base, tokenizer, pretrain_mm_adapter = args.pretrain_mm_adapter)
    image_processor = base.get_vision_tower().image_processor

    # é…ç½® LoRAï¼Œè®¾ç½®å…¶è¶…å‚æ•° r, lora_alpha, target_modules, lora_dropout ç­‰ã€‚å¦‚æžœå¯ç”¨äº† LoRAï¼Œåˆ™ä½¿ç”¨è¯¥é…ç½®æ›´æ–°åŸºç¡€æ¨¡åž‹ã€‚
    base_lora_config = LoraConfig(
            r=128,
            lora_alpha=256,
            target_modules=find_all_linear_names(base,args.train_vision),
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
    if args.use_lora:
        base = get_peft_model(base, base_lora_config)


    ## Inputing Prompt here
    ###############
    ## å®žä¾‹åŒ–çŽ¯å¢ƒ
    ###############
    assert args.alf_config is not None, "Alfworld environment requires a config file"
    print(f"\033[33mCreating Env: {args.alf_config}\033[0m")
    print(f"\033[33mPath: {os.getenv('ALFWORLD_DATA')}\033[0m")
    envs = AlfEnv(args.alf_config)
    obs, infos = envs.reset(seed=args.seed)
    admissible_commands = list(infos['admissible_commands'])[0]


    #################### Traj Storage ####################
    # jkc0904
    trajs = TrajBuffer(training_args.max_pairs, args.num_processes, training_args.max_history_tokens, args.max_new_tokens, (300, 300, 3), history_horizon=training_args.history_horizon)

    trajs.add_test_state(tokenizer)
    # print(f"\033[41m{type(infos['observation_text'])}: {infos['observation_text']}\033[0m")
    trajs.start_traj(infos['observation_text'][0])

    #################### Traj Storage End ####################


    # ç”Ÿæˆæç¤ºè¯ @TODO:éœ€è¦ä¿®æ”¹æç¤ºè¯, åŠ å…¥åŽ†å²æ•°æ®ðŸŒŸ
    history = trajs.get_history_data()
    # qs = get_alfworld_prompt(envs, obs = infos['observation_text'], admissible_actions=admissible_commands, action_only = args.action_only_prompt)
    qs = get_dpo_prompt(envs, obs = infos['observation_text'], admissible_actions=admissible_commands, history=history, action_only = args.action_only_prompt)
    qs = DEFAULT_IMAGE_TOKEN + "\n" + qs
    conv = conv_templates[args.conv_mode].copy()  # ä½¿ç”¨å¯¹è¯æ¨¡æ¿æž„å»ºå¯¹è¯å¹¶ç”Ÿæˆæœ€ç»ˆçš„æç¤ºæ–‡æœ¬ã€‚
    conv.append_message(conv.roles[0], qs)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    print(f"\033[34m{prompt}\033[0m")

    # ä½¿ç”¨ tokenizer_image_token å‡½æ•°å°†æç¤ºæ–‡æœ¬è½¬åŒ–ä¸ºè¾“å…¥ IDï¼Œè¿”å›žå¼ é‡æ ¼å¼ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰é›¶å€¼ä½ç½®éƒ½è¢«æ›¿æ¢ä¸ºç‰¹å®šçš„æ ‡è®°ï¼ˆ259ï¼‰ã€‚
    INPUT_IDS = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0)
    INPUT_IDS[INPUT_IDS == 0] = 259 # 869: . (period), 29871: SPIECE, 259: whitespace

    if "alfred" in args.env_name.lower():
        projection_f = partial(lambda x: x)

    policy_model = DPOPolicy(tokenizer=tokenizer,
                             image_processor=image_processor,
                             base=base,
                             projection_f=projection_f,
                             INPUT_IDS=INPUT_IDS,
                             args=args)
    ref_model = copy.deepcopy(policy_model) ##TODO
    for param in ref_model.parameters():
        param.requires_grad = False

    optimizer = optim.Adam(policy_model.base.parameters(), lr=args.init_lr, eps=args.eps, weight_decay=args.weight_decay)  # ä½™å¼¦é€€ç«å­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼Œéšç€è®­ç»ƒè¿‡ç¨‹é€æ¸å‡å°‘å­¦ä¹ çŽ‡ã€‚
    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.lr_max_steps, eta_min=args.end_lr)
    AcceleratorState().deepspeed_plugin.deepspeed_config['train_micro_batch_size_per_gpu'] = 1  # è®¾ç½® DeepSpeed çš„è®­ç»ƒå¾®æ‰¹å¤§å°ä¸º 1ã€‚

    policy_model, optimizer, lr_scheduler = accelerator.prepare(policy_model, optimizer, lr_scheduler) ##TODO

    #################################################################
    # åˆ›å»º DPOï¼ˆDirect Preference Optimizationï¼‰ä»£ç†ï¼Œç”¨äºŽå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ä¼˜åŒ–ã€‚
    #################################################################
    agent = algo.DPO(
            policy_model,
            ref_model,
            optimizer,
            accelerator,
            training_args.beta,
            args.ppo_epoch,
            args.mini_batch_size,
            args.max_grad_norm,
            training_args.label_smoothing,
            training_args.reference_free
            )


    ######################################## fzr TODO ########################################
    ## åˆ›å»ºä¸€ä¸ª RolloutStorage å®žä¾‹ï¼Œç”¨äºŽå­˜å‚¨å›žåˆæ•°æ®ï¼Œå‚æ•°åŒ…æ‹¬æ­¥æ•°ã€è¿›ç¨‹æ•°ã€è§‚å¯Ÿç©ºé—´ã€åŠ¨ä½œç©ºé—´å’Œæœ€å¤§æ–°æ ‡è®°æ•°é‡ã€‚
    # rollouts = RolloutStorage(args.num_steps, args.num_processes,
    #                           (300, 300, 3), spaces.Discrete(14), args.max_new_tokens)


    image_tensor = obs
    last_step_obs = copy.deepcopy(obs)  # è®°å¾—æ›´æ–°ï¼ï¼ðŸŒŸ

    ## æ‰§è¡Œæ¨¡åž‹çš„ act å‡½æ•°ï¼ŒåŸºäºŽè¾“å…¥å›¾åƒå¼ é‡å’Œè¾“å…¥ ID ç”ŸæˆåŠ¨ä½œå’Œç›¸å…³çš„æ¦‚çŽ‡ä¿¡æ¯ï¼Œå¹¶èŽ·å–å¯è¡Œå‘½ä»¤ã€‚
    output_ids, action, action_log_prob, action_tokens_log_prob = policy_model.act(image_tensor, INPUT_IDS = INPUT_IDS)
    admissible_commands = list(infos['admissible_commands'])[0]

    print(f"\033[34moutput_ids:\033[0m{output_ids}")
    print(f"\033[34mprompt:\033[0m{prompt}")
    print(f"\033[34maction:\033[0m{action}")
    print(f"\033[34maction_log_prob:\033[0m{action_log_prob}")
    print(f"\033[34maction_tokens_log_prob:\033[0m{action_tokens_log_prob}")


    # åˆå§‹åŒ–å¤šä¸ªåŒç«¯é˜Ÿåˆ—ï¼Œç”¨äºŽå­˜å‚¨æ¯ä¸ªå›žåˆçš„å¥–åŠ±ã€æˆåŠŸçŽ‡ã€åŠ¨ä½œæ ‡è®°æ—¥å¿—æ¦‚çŽ‡ç­‰ä¿¡æ¯ï¼Œé˜Ÿåˆ—é•¿åº¦ä¸ºæ¯ä¸ªå›žåˆæœ€å¤§è¯„ä¼°æ¬¡æ•°ã€‚
    episode_rewards = deque(maxlen=args.eval_num_per_episode)
    episode_success_rate = deque(maxlen=args.eval_num_per_episode)
    episode_gc_success_rate = deque(maxlen=args.eval_num_per_episode)
    episode_succ_rate_pick_and_place = deque(maxlen=args.eval_num_per_episode)
    episode_succ_rate_pick_two_obj_and_place = deque(maxlen=args.eval_num_per_episode)
    episode_succ_rate_look_at_obj_in_light = deque(maxlen=args.eval_num_per_episode)
    episode_succ_rate_pick_heat_then_place_in_recep = deque(maxlen=args.eval_num_per_episode)
    episode_succ_rate_pick_cool_then_place_in_recep = deque(maxlen=args.eval_num_per_episode)
    episode_succ_rate_pick_clean_then_place_in_recep = deque(maxlen=args.eval_num_per_episode)
    episode_action_tokens_log_prob = deque(maxlen=args.eval_num_per_episode)



    ##########################################################################################
    ######################################## å¼€å§‹è®­ç»ƒ ########################################
    ##########################################################################################
    # è®°å½•å¼€å§‹æ—¶é—´ï¼Œè®¡ç®—è®­ç»ƒä¸­çš„æ›´æ–°æ¬¡æ•°ã€‚å¦‚æžœä½¿ç”¨ wandbï¼ˆWeights and Biasesï¼‰è¿›è¡Œå®žéªŒè¿½è¸ªï¼Œåˆå§‹åŒ– wandbï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç”¨äºŽè®°å½•æ–‡æœ¬æ•°æ®çš„è¡¨æ ¼ã€‚
    start = time.time()
    num_updates = int(
        args.num_env_steps) // args.num_steps // args.num_processes
    if args.use_wandb:
        import wandb
        run_name = args.wandb_run + "-" + args.env_name
        wandb.init(project=args.wandb_project, name=run_name, group=run_name, config=args)
        text_table = wandb.Table(columns=["epoch", "obs_text", "text_action"])
    # print(f"\033[44mprompt\033[34m:{prompt}\033[0m")
    running_episode_rewards = torch.zeros(args.num_processes).flatten()

    ### ä¸»å¾ªçŽ¯
    rs = []
    for j in tqdm(range(num_updates)):

        for step in tqdm(range(args.num_steps)):
            print(f"\033[31mstep {step} in {args.num_steps} total\033[0m")
            # Sample actions
            with torch.no_grad():
                INPUT_IDS = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0)
                output_id, action, action_log_prob, action_tokens_log_prob = policy_model.act(last_step_obs, INPUT_IDS = INPUT_IDS)  # TODO

                admissible_commands = list(infos['admissible_commands'])[0]
            text_action = tokenizer.decode(list(filter(lambda num: num != 0, output_id[0].tolist())))

            # Observation, reward and next obs
            # æ‰§è¡ŒåŠ¨ä½œï¼ŒèŽ·å–æ–°è§‚å¯Ÿã€å¥–åŠ±ã€å®Œæˆæ ‡å¿—å’Œä¿¡æ¯ã€‚å¦‚æžœçŽ¯å¢ƒåç§°åŒ…å« alfredï¼Œåˆ™é‡æ–°ç”Ÿæˆæç¤ºã€‚
            obs, reward, done, infos = envs.step(action) # for alf this will already process action
            last_step_obs = copy.deepcopy(obs) # æ›´æ–°last_obsðŸŒŸ
            # print(f"\033[32mReward: {reward}\033[0m")
            if "alfred" in args.env_name.lower():
                admissible_commands = list(infos['admissible_commands'])[0]
                history = trajs.get_history_data()  # jkc0904
                # qs = get_alfworld_prompt(envs, obs = infos['observation_text'], admissible_actions=admissible_commands, action_only = args.action_only_prompt)
                qs = get_dpo_prompt(envs, obs = infos['observation_text'], admissible_actions=admissible_commands, history=history, action_only = args.action_only_prompt)
                qs = DEFAULT_IMAGE_TOKEN + "\n" + qs
                conv = conv_templates[args.conv_mode].copy()
                conv.append_message(conv.roles[0], qs)
                conv.append_message(conv.roles[1], None)
                prompt = conv.get_prompt()

            #################### Traj Storage ####################
            ######################################## fzr TODO ########################################
            # trajs.add_point(task_name, traj_name, {"prompt": prompt, "obs": infos['observation_text'], "act": action, "preference": copy.deepcopy(infos['goal_condition_success_rate'][0])})
            # if (args.num_steps * j + step) % 10 == 0:
            #     print(f"\033[44m{trajs}\033[0m")
            #     trajs.save_to_file(f"./trajs/{task_name}.pkl")


            masks = torch.FloatTensor(
                [[0.0] if done_ else [1.0] for done_ in done])  # åˆ›å»ºæŽ©ç å¼ é‡ï¼Œç”¨äºŽæŒ‡ç¤ºæ˜¯å¦ç»“æŸå½“å‰å›žåˆã€‚
            
            # æ›´æ–°ç´¯ç§¯å¥–åŠ±ã€‚å¦‚æžœå›žåˆç»“æŸï¼Œè®°å½•æ¯ä¸ªä»»åŠ¡çš„æˆåŠŸçŽ‡ï¼Œå¹¶é‡ç½®å›žåˆã€‚
            running_episode_rewards += reward.flatten()
            for i, d, r in zip(range(args.num_processes), done, reward):
                if d:
                    episode_rewards.append(running_episode_rewards[i].item())
                    # record success rate of different types of tasks
                    if "pick_and_place" in infos["extra.gamefile"][0]:
                        episode_succ_rate_pick_and_place.append(float(infos['won'][0]))
                    elif "pick_two_obj_and_place" in infos["extra.gamefile"][0]:
                        episode_succ_rate_pick_two_obj_and_place.append(float(infos['won'][0]))
                    elif "look_at_obj_in_light" in infos["extra.gamefile"][0]:
                        episode_succ_rate_look_at_obj_in_light.append(float(infos['won'][0]))
                    elif "pick_heat_then_place_in_recep" in infos["extra.gamefile"][0]:
                        episode_succ_rate_pick_heat_then_place_in_recep.append(float(infos['won'][0]))
                    elif "pick_cool_then_place_in_recep" in infos["extra.gamefile"][0]:
                        episode_succ_rate_pick_cool_then_place_in_recep.append(float(infos['won'][0]))
                    elif "pick_clean_then_place_in_recep" in infos["extra.gamefile"][0]:
                        episode_succ_rate_pick_clean_then_place_in_recep.append(float(infos['won'][0]))
                    # record the final success rate
                    episode_success_rate.append(float(infos['won'][0]))
                    episode_gc_success_rate.append(float(infos['goal_condition_success_rate'][0]))
                    print(len(episode_success_rate))
                    episode_action_tokens_log_prob.append(action_tokens_log_prob[i].item())
                    running_episode_rewards[i] = 0
                    obs, infos = envs.reset(seed=args.seed)
                    # print(f"\033[34mreset! {infos}\033[0m")
                    # return 0

                    ######################################## fzr TODO ########################################
                    #################### Traj Storage ####################
                    trajs.start_traj(infos['observation_text'][0])
                    # é‡ç½®è½¨è¿¹å­˜å‚¨ðŸŒŸ


                    # é‡ç½®çŽ¯å¢ƒåŽï¼Œé‡æ–°ç”Ÿæˆæç¤ºã€‚
                    admissible_commands = list(infos['admissible_commands'])[0]
                    history = trajs.get_history_data()  # jkc0904
                    # qs = get_alfworld_prompt(envs, obs = infos['observation_text'], admissible_actions=admissible_commands, action_only = args.action_only_prompt)
                    qs = get_dpo_prompt(envs, obs = infos['observation_text'], admissible_actions=admissible_commands, history=history, action_only = args.action_only_prompt)
                    qs = DEFAULT_IMAGE_TOKEN + "\n" + qs
                    conv = conv_templates[args.conv_mode].copy()
                    conv.append_message(conv.roles[0], qs)
                    conv.append_message(conv.roles[1], None)
                    prompt = conv.get_prompt()
                    
            
            # åˆ›å»º bad_masks å¼ é‡ï¼Œå¹¶ç¡®å®šåŠ¨ä½œ IDï¼ˆåœ¨å½“å‰ä»£ç ä¸­æœªä½¿ç”¨ï¼‰ã€‚
            # bad_masks is a legact implementation in the storage
            bad_masks = torch.zeros(args.num_processes, 1)
            # action_id is also a legacy implementation in the storage, it is never used in the PPO update
            action_id = None
            for i in range(len(admissible_commands)):
                if admissible_commands[i] == action:
                    action_id = i
                    break
            if not action_id:
                action_id = 0
            action_id = torch.tensor(action_id)

            ######################################## fzr TODO ########################################
            trajs.add_new_state(obs, infos['observation_text'][0], text_action, float(infos['goal_condition_success_rate'][0]), prompt=None)
            # rollouts.insert(obs, output_id, action_id,
            #                     action_log_prob, value, reward, masks, bad_masks)  # å°†å½“å‰è§‚å¯Ÿã€è¾“å‡º IDã€åŠ¨ä½œ IDã€æ—¥å¿—æ¦‚çŽ‡ã€ä»·å€¼ã€å¥–åŠ±ã€æŽ©ç å’Œ bad_masks æ’å…¥åˆ°å›žåˆå­˜å‚¨ä¸­ã€‚

        print(f"\033[43mUpdates:{j}\033[0m")
        print(f"\033[33mprompt:\033[0m{prompt}")
        print(f"\033[33maction_log_prob:\033[0m{action_log_prob}")
        print(f"\033[33mtext_action:\033[0m{text_action}")
        print(f"\033[33maction:\033[0m{action}")
        print(f"\033[33mground truth:\033[0m{infos}")
        print(f"\033[33msuccess_rate:\033[0m{np.mean(episode_success_rate)}")


        ##### ä½¿ç”¨ DPO ç®—æ³•æ›´æ–°ç­–ç•¥ï¼Œè®¡ç®—ä»·å€¼å’ŒåŠ¨ä½œæŸå¤±ä»¥åŠç­–ç•¥çš„ç†µã€‚å¹¶æ›´æ–°å­¦ä¹ çŽ‡è°ƒåº¦å™¨ã€‚#####
        # rollouts.compute_returns(next_value, args.use_gae, args.gamma,
        #                          args.gae_lambda, args.use_proper_time_limits)
        
        if trajs.valid_pairs >= training_args.start_training_pair_nums:
            action_loss = agent.update(trajs)
            lr_scheduler.step()


            # æ›´æ–°åŽçš„å›žåˆå­˜å‚¨ã€‚æ‰“å°æ›´æ–°çŠ¶æ€ï¼ŒåŒ…æ‹¬å¥–åŠ±ã€æˆåŠŸçŽ‡å’Œå…¶ä»–ç»Ÿè®¡ä¿¡æ¯ã€‚å¦‚æžœä½¿ç”¨ wandbï¼Œåˆ™è®°å½•å½“å‰è¿­ä»£çš„ç›¸å…³æ•°æ®ã€‚
            ######################################## fzr TODO ########################################
            # rollouts.after_update() # TODO
            if len(episode_rewards) > 1:

                try:
                    rs.append(episode_success_rate)
                    np.save("./rewarddddddddddddddd.npy", np.array(rs))
                except Exception as e:
                    for _ in range(5):
                        print(f"\033[31m###############################\033[0m")
                    print(f"\033[43m{e}\033[0m")
                    for _ in range(5):
                        print(f"\033[31m###############################\033[0m")


                total_num_steps = (j + 1) * args.num_processes * args.num_steps
                end = time.time()
                print(
                    "\033[32mUpdates {}, num timesteps {}, FPS {} \n Last {} training episodes: mean/median reward {:.2f}/{:.2f}, min/max reward {:.2f}/{:.2f}, success_rate {:.2f}, loss: {:.2f}\n\033[0m"
                    .format(j, total_num_steps,
                            int(total_num_steps / (end - start)),
                            len(episode_rewards), np.mean(episode_rewards),
                            np.median(episode_rewards), np.min(episode_rewards),
                            np.max(episode_rewards), np.mean(episode_success_rate),
                            action_loss))
                if args.use_wandb:
                    wandb_images = [wandb.Image(image.cpu().numpy()) for image in obs]
                    text_table.add_data(j, infos['observation_text'][0], text_action)
                    wandb.log({"iteration": j,
                            "num_timesteps": total_num_steps,
                            "FPS": int(total_num_steps / (end - start)),
                            "episode_reward.mean": np.mean(episode_rewards),
                            "episode_reward.median": np.median(episode_rewards),
                            "episode_reward.min": np.min(episode_rewards),
                            "episode_reward.max": np.max(episode_rewards),
                            "episode_success_rate.mean": np.mean(episode_success_rate),
                            "episode_action_tokens_log_prob.mean": np.mean(episode_action_tokens_log_prob),
                            "episode_(goal_condition)_success_rate.mean": np.mean(episode_gc_success_rate),
                            "episode_succ_rate_pick_and_place.mean": np.mean(episode_succ_rate_pick_and_place),
                            "episode_succ_rate_pick_two_obj_and_place.mean": np.mean(episode_succ_rate_pick_two_obj_and_place),
                            "episode_succ_rate_look_at_obj_in_light.mean": np.mean(episode_succ_rate_look_at_obj_in_light),
                            "episode_succ_rate_pick_heat_then_place_in_recep.mean": np.mean(episode_succ_rate_pick_heat_then_place_in_recep),
                            "episode_succ_rate_pick_cool_then_place_in_recep.mean": np.mean(episode_succ_rate_pick_cool_then_place_in_recep),
                            "episode_succ_rate_pick_clean_then_place_in_recep.mean": np.mean(episode_succ_rate_pick_clean_then_place_in_recep),
                            "episode_num": len(episode_success_rate),
                            # "distribution_entropy": dist_entropy,
                            "text": text_table,
                            "image": wandb_images,
                            # "value.loss": value_loss,
                            "action.loss": action_loss,
                            "action_log_prob": action_log_prob.to('cpu').float().numpy()[0],
                            # "reward.max": rollouts.rewards.max().item(),
                            # "reward.min": rollouts.rewards.min().item(),
                            # "reward.mean": rollouts.rewards.mean().item(),
                            # "reward.std": rollouts.rewards.std().item(),
                            # "reward.median": rollouts.rewards.median().item(),
                            # "return.max": rollouts.returns.max().item(),
                            # "return.min": rollouts.returns.min().item(),
                            # "return.mean": rollouts.returns.mean().item(),
                            # "return.std": rollouts.returns.std().item(),
                            # "value.max": rollouts.value_preds.max().item(),
                            # "value.min": rollouts.value_preds.min().item(),
                            # "value.mean": rollouts.value_preds.mean().item(),
                            # "value.std": rollouts.value_preds.std().item(),
                            })
        else:
            print(f"\033[43m!!!Not Enough Pairs!!!\033[0m")



if __name__ == "__main__":
    main()
