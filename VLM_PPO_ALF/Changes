# MODIFIED a2c_ppo_acktr/storage.py: added class TrajStorage for in-order storage of sampled trajectories. Search KEY: 240825tra

@20240829
# MKDIR DPO under DPO4VLM/VLM_PPO_ALF to save DPOTrainer codes
# COPY stepdpo_trainer into directory DPO
# MKFILE configs.py
      add functions: H4ArgumentParser, ModelArguments, DataArguments, RLArguments, DPOConfig, StepDPOConfig
# MODIFIED main_alf.py: add "import configs"
# COMMENTOUT main_alf.py args=get_args()  >>>注意，accelerate的config和程序传入的不是一个


@20240830
# MODIFIED a2c_ppo_acktr/storage.py: add function "to_dataset_dict" to class TrajStorage. KEY: 240830dic
# MODIFIED configs.py: 在parse_yaml_and_args中新增命令行参数解析格式
# MODIFIED a2c_ppo_acktr/llava_interface/interface.py: 将llava函数去除掉value部分，用于适配DPO🌟🌟
# MODIFIED a2c_ppo_acktr/model.py: 添加DPOPolicy类，舍弃critic结构，直接输出策略🌟🌟

@20240903
# DELETE a2c_ppo_acktr/storage.py: TrajStorage
# MODIFIED a2c_ppo_acktr/storage.py: add TrajBuffer

@20240904
# MODIFIED main_alf.py: add "base = base.to(model_device)"
# MODIFIED main_alf.py: 修改rollout为trajbuffer, 更新了参数存储逻辑
# MODIFIED ppo.py & interface.py: 将base=policy_model_base改成base=policy_model, 根据trajsbuffer修改了相关逻辑

