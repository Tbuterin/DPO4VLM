# MODIFIED a2c_ppo_acktr/storage.py: added class TrajStorage for in-order storage of sampled trajectories. Search KEY: 240825tra

@20240829
# MKDIR DPO under DPO4VLM/VLM_PPO_ALF to save DPOTrainer codes
# COPY stepdpo_trainer into directory DPO
# MKFILE configs.py
      add functions: H4ArgumentParser, ModelArguments, DataArguments, RLArguments, DPOConfig, StepDPOConfig
# MODIFIED main_alf.py: add "import configs"
# COMMENTOUT main_alf.py args=get_args()  >>>æ³¨æ„ï¼Œaccelerateçš„configå’Œç¨‹åºä¼ å…¥çš„ä¸æ˜¯ä¸€ä¸ª


@20240830
# MODIFIED a2c_ppo_acktr/storage.py: add function "to_dataset_dict" to class TrajStorage. KEY: 240830dic
# MODIFIED configs.py: åœ¨parse_yaml_and_argsä¸­æ–°å¢å‘½ä»¤è¡Œå‚æ•°è§£ææ ¼å¼
# MODIFIED a2c_ppo_acktr/llava_interface/interface.py: å°†llavaå‡½æ•°å»é™¤æ‰valueéƒ¨åˆ†ï¼Œç”¨äºé€‚é…DPOğŸŒŸğŸŒŸ
# MODIFIED a2c_ppo_acktr/model.py: æ·»åŠ DPOPolicyç±»ï¼Œèˆå¼ƒcriticç»“æ„ï¼Œç›´æ¥è¾“å‡ºç­–ç•¥ğŸŒŸğŸŒŸ

@20240903
# DELETE a2c_ppo_acktr/storage.py: TrajStorage
# MODIFIED a2c_ppo_acktr/storage.py: add TrajBuffer

@20240904
# MODIFIED main_alf.py: add "base = base.to(model_device)"
# MODIFIED main_alf.py: ä¿®æ”¹rolloutä¸ºtrajbuffer, æ›´æ–°äº†å‚æ•°å­˜å‚¨é€»è¾‘
# MODIFIED ppo.py & interface.py: å°†base=policy_model_baseæ”¹æˆbase=policy_model, æ ¹æ®trajsbufferä¿®æ”¹äº†ç›¸å…³é€»è¾‘

